{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm import tqdm\n",
    "from collections import deque\n",
    "\n",
    "from functions.game import play\n",
    "from functions.game import play_step\n",
    "from functions.utils import generate_board\n",
    "from functions.utils import boards_to_array\n",
    "from functions.players import trained_player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DISCOUNT_FACTOR = 0.99\n",
    "NUM_EPISODES = 10_000\n",
    "SOLVED_SCORE = 120\n",
    "MAX_STEPS = 41\n",
    "EPS = 0.2\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, observation_space, action_space):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.layer1 = nn.Linear(observation_space, 64)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(64, 32)\n",
    "        self.bn2 = nn.BatchNorm1d(32)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.output_layer = nn.Linear(32, action_space)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.output_layer(x)\n",
    "        x = self.softmax(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StateValueNetwork(nn.Module):\n",
    "    def __init__(self, observation_space):\n",
    "        super(StateValueNetwork, self).__init__()\n",
    "        self.layer1 = nn.Linear(observation_space, 64)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(64, 32)\n",
    "        self.bn2 = nn.BatchNorm1d(32)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.output_layer = nn.Linear(32, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.output_layer(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_network_X = PolicyNetwork(90, 81).to(DEVICE)\n",
    "stateval_network_X = StateValueNetwork(90).to(DEVICE)\n",
    "\n",
    "policy_optimizer_X = optim.SGD(policy_network_X.parameters(), lr=0.001)\n",
    "stateval_optimizer_X = optim.SGD(stateval_network_X.parameters(), lr=0.001)\n",
    "\n",
    "policy_network_O = PolicyNetwork(90, 81).to(DEVICE)\n",
    "stateval_network_O = StateValueNetwork(90).to(DEVICE)\n",
    "\n",
    "policy_optimizer_O = optim.SGD(policy_network_O.parameters(), lr=0.001)\n",
    "stateval_optimizer_O = optim.SGD(stateval_network_O.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rl_player_X(all_board, global_board, i, j):\n",
    "    next_action_basis = (i, j)\n",
    "    return trained_player(all_board, global_board, next_action_basis, policy_network_X)\n",
    "\n",
    "def rl_player_O(all_board, global_board, i, j):\n",
    "    next_action_basis = (i, j)\n",
    "    return trained_player(all_board, global_board, next_action_basis, policy_network_O)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_netwrok(policy_network, stateval_network, policy_optimizer, stateval_optimizer, player, rl_player):\n",
    "    recent_scores = deque(maxlen = 100)\n",
    "    rival = (player + 1) % 2\n",
    "    for episode in tqdm(range(NUM_EPISODES)):\n",
    "        all_board, global_board = generate_board()\n",
    "        next_action_basis = (None, None)\n",
    "        if player == 1:\n",
    "            i, j = next_action_basis\n",
    "            next_action = rl_player(all_board, global_board, i, j)\n",
    "            all_board, global_board, _, next_action_basis = play_step(all_board, global_board, next_action, player=rival)\n",
    "\n",
    "        state = boards_to_array(all_board, global_board)\n",
    "        done = False\n",
    "        score = 0\n",
    "        I = 1\n",
    "        \n",
    "        for step in range(MAX_STEPS):\n",
    "            action, lp = trained_player(all_board, global_board, next_action_basis, policy_network, eps=EPS, training=True)\n",
    "            \n",
    "            all_board, global_board, reward, next_action_basis = play_step(all_board, global_board, action, player=player)\n",
    "            new_state = boards_to_array(all_board, global_board)\n",
    "            score += reward\n",
    "            \n",
    "            state_tensor = torch.from_numpy(state).float().unsqueeze(0).to(DEVICE)\n",
    "            state_val = stateval_network(state_tensor)\n",
    "            \n",
    "            new_state_tensor = torch.from_numpy(new_state).float().unsqueeze(0).to(DEVICE)        \n",
    "            new_state_val = stateval_network(new_state_tensor)\n",
    "            \n",
    "            done = reward != 1\n",
    "            if not done:\n",
    "                i, j = next_action_basis\n",
    "                next_action = rl_player(all_board, global_board, i, j)\n",
    "                all_board, global_board, other_reward, next_action_basis = play_step(all_board, global_board, next_action, player=rival)\n",
    "                \n",
    "                new_state = boards_to_array(all_board, global_board)\n",
    "                if other_reward == 100: reward = -50\n",
    "                else: reward = other_reward\n",
    "                score += reward\n",
    "                done = reward != 1\n",
    "            \n",
    "            if done: new_state_val = torch.tensor([0]).float().unsqueeze(0).to(DEVICE)\n",
    "            val_loss = F.mse_loss(reward + DISCOUNT_FACTOR * new_state_val, state_val)\n",
    "            val_loss *= I\n",
    "            \n",
    "            advantage = reward + DISCOUNT_FACTOR * new_state_val.item() - state_val.item()\n",
    "            policy_loss = -lp * advantage\n",
    "            policy_loss *= I\n",
    "            \n",
    "            policy_optimizer.zero_grad()\n",
    "            policy_loss.backward(retain_graph=True)\n",
    "            policy_optimizer.step()\n",
    "            \n",
    "            stateval_optimizer.zero_grad()\n",
    "            val_loss.backward()\n",
    "            stateval_optimizer.step()\n",
    "            \n",
    "            if done: break\n",
    "            state = new_state\n",
    "            I *= DISCOUNT_FACTOR\n",
    "        \n",
    "        recent_scores.append(score)\n",
    "        if np.array(recent_scores).mean() >= SOLVED_SCORE and len(recent_scores) == 100: break\n",
    "    \n",
    "    return policy_network, stateval_network, policy_optimizer, stateval_optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 99/10000 [00:02<04:06, 40.10it/s]\n",
      "  2%|▏         | 170/10000 [00:03<03:35, 45.58it/s]\n"
     ]
    }
   ],
   "source": [
    "policy_network_O, stateval_network_O, policy_optimizer_O, stateval_optimizer_O = train_netwrok(policy_network_O,\n",
    "                                                                                               stateval_network_O,\n",
    "                                                                                               policy_optimizer_O,\n",
    "                                                                                               stateval_optimizer_O,\n",
    "                                                                                               1,\n",
    "                                                                                               rl_player_X)\n",
    "\n",
    "policy_network_X, stateval_network_X, policy_optimizer_X, stateval_optimizer_X = train_netwrok(policy_network_X,\n",
    "                                                                                               stateval_network_X,\n",
    "                                                                                               policy_optimizer_X,\n",
    "                                                                                               stateval_optimizer_X,\n",
    "                                                                                               0,\n",
    "                                                                                               rl_player_O)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "games = 10_000\n",
    "result = ['X won', 'draw', 'O won']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [01:07<00:00, 147.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X won: 68.46%\n",
      "draw: 11.05%\n",
      "O won: 20.49%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results = [play(player_0=rl_player_X) for _ in tqdm(range(games))]\n",
    "results, counts = np.unique(results, return_counts = True)\n",
    "counts = counts / np.sum(counts)\n",
    "\n",
    "p = list()\n",
    "for r in result:\n",
    "    inx = np.argmax(results == r)\n",
    "    print(f'{r}: {counts[inx]:.2%}')\n",
    "    p.append(counts[inx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [01:04<00:00, 155.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X won: 28.26%\n",
      "draw: 12.48%\n",
      "O won: 59.26%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results = [play(player_1=rl_player_O) for _ in tqdm(range(games))]\n",
    "results, counts = np.unique(results, return_counts = True)\n",
    "counts = counts / np.sum(counts)\n",
    "\n",
    "p = list()\n",
    "for r in result:\n",
    "    inx = np.argmax(results == r)\n",
    "    print(f'{r}: {counts[inx]:.2%}')\n",
    "    p.append(counts[inx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:32<00:00, 311.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X won: 41.01%\n",
      "draw: 22.52%\n",
      "O won: 36.47%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results = [play() for _ in tqdm(range(games))]\n",
    "results, counts = np.unique(results, return_counts = True)\n",
    "counts = counts / np.sum(counts)\n",
    "\n",
    "p = list()\n",
    "for r in result:\n",
    "    inx = np.argmax(results == r)\n",
    "    print(f'{r}: {counts[inx]:.2%}')\n",
    "    p.append(counts[inx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_scripted = torch.jit.script(policy_network_X)\n",
    "model_scripted.save('models/policy_X_1.pt')\n",
    "\n",
    "model_scripted = torch.jit.script(policy_network_O)\n",
    "model_scripted.save('models/policy_O_1.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12 (default, Apr 25 2023, 18:29:14) \n[Clang 14.0.3 (clang-1403.0.22.14.1)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "68a818ea200cf26246555ed215b2805781440e4bd9c1dd7c5d891140b3a1e7c0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
