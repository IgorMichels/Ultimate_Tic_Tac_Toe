{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm import tqdm\n",
    "from collections import deque\n",
    "\n",
    "from game import play\n",
    "from game import play_step\n",
    "from utils import generate_board\n",
    "from utils import boards_to_array\n",
    "from players import trained_player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DISCOUNT_FACTOR = 0.99\n",
    "NUM_EPISODES = 10_000\n",
    "SOLVED_SCORE = 120\n",
    "MAX_STEPS = 41\n",
    "EPS = 0.2\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, observation_space, action_space):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.layer1 = nn.Linear(observation_space, 64)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(64, 32)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.output_layer = nn.Linear(32, action_space)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.output_layer(x)\n",
    "        x = self.softmax(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StateValueNetwork(nn.Module):\n",
    "    def __init__(self, observation_space):\n",
    "        super(StateValueNetwork, self).__init__()\n",
    "        self.layer1 = nn.Linear(observation_space, 64)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(64, 32)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.output_layer = nn.Linear(32, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.output_layer(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_network = PolicyNetwork(90, 81).to(DEVICE)\n",
    "stateval_network = StateValueNetwork(90).to(DEVICE)\n",
    "\n",
    "policy_optimizer = optim.SGD(policy_network.parameters(), lr=0.001)\n",
    "stateval_optimizer = optim.SGD(stateval_network.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [04:37<00:00, 35.99it/s]\n"
     ]
    }
   ],
   "source": [
    "recent_scores = deque(maxlen = 100)\n",
    "\n",
    "for episode in tqdm(range(NUM_EPISODES)):\n",
    "    all_board, global_board = generate_board()\n",
    "    state = boards_to_array(all_board, global_board)\n",
    "    next_action_basis = (None, None)\n",
    "    done = False\n",
    "    score = 0\n",
    "    I = 1\n",
    "    \n",
    "    for step in range(MAX_STEPS):\n",
    "        action, lp = trained_player(all_board, global_board, next_action_basis, policy_network, eps=EPS, training=True)\n",
    "        \n",
    "        all_board, global_board, reward, next_action_basis = play_step(all_board, global_board, action, player = 0)\n",
    "        new_state = boards_to_array(all_board, global_board)\n",
    "        score += reward\n",
    "        \n",
    "        state_tensor = torch.from_numpy(state).float().unsqueeze(0).to(DEVICE)\n",
    "        state_val = stateval_network(state_tensor)\n",
    "        \n",
    "        new_state_tensor = torch.from_numpy(new_state).float().unsqueeze(0).to(DEVICE)        \n",
    "        new_state_val = stateval_network(new_state_tensor)\n",
    "        \n",
    "        done = reward != 1\n",
    "        if not done:\n",
    "            all_board, global_board, other_reward, next_action_basis = play_step(all_board, global_board, next_action_basis, player = 1)\n",
    "            new_state = boards_to_array(all_board, global_board)\n",
    "            if other_reward == 100: reward = -50\n",
    "            else: reward = other_reward\n",
    "            score += reward\n",
    "            done = reward != 1\n",
    "        \n",
    "        if done: new_state_val = torch.tensor([0]).float().unsqueeze(0).to(DEVICE)\n",
    "        val_loss = F.mse_loss(reward + DISCOUNT_FACTOR * new_state_val, state_val)\n",
    "        val_loss *= I\n",
    "        \n",
    "        advantage = reward + DISCOUNT_FACTOR * new_state_val.item() - state_val.item()\n",
    "        policy_loss = -lp * advantage\n",
    "        policy_loss *= I\n",
    "        \n",
    "        policy_optimizer.zero_grad()\n",
    "        policy_loss.backward(retain_graph=True)\n",
    "        policy_optimizer.step()\n",
    "        \n",
    "        stateval_optimizer.zero_grad()\n",
    "        val_loss.backward()\n",
    "        stateval_optimizer.step()\n",
    "        \n",
    "        if done: break\n",
    "        state = new_state\n",
    "        I *= DISCOUNT_FACTOR\n",
    "    \n",
    "    recent_scores.append(score)\n",
    "    if np.array(recent_scores).mean() >= SOLVED_SCORE and len(recent_scores) == 100: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rl_player(all_board, global_board, i, j):\n",
    "    next_action_basis = (i, j)\n",
    "    return trained_player(all_board, global_board, next_action_basis, policy_network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "games = 10_000\n",
    "result = ['X won', 'draw', 'O won']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [01:26<00:00, 115.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X won: 39.01%\n",
      "draw: 22.11%\n",
      "O won: 38.88%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results = [play(player_0=rl_player) for _ in tqdm(range(games))]\n",
    "results, counts = np.unique(results, return_counts = True)\n",
    "counts = counts / np.sum(counts)\n",
    "\n",
    "p = list()\n",
    "for r in result:\n",
    "    inx = np.argmax(results == r)\n",
    "    print(f'{r}: {counts[inx]:.2%}')\n",
    "    p.append(counts[inx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:31<00:00, 320.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X won: 41.10%\n",
      "draw: 22.21%\n",
      "O won: 36.69%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results = [play() for _ in tqdm(range(games))]\n",
    "results, counts = np.unique(results, return_counts = True)\n",
    "counts = counts / np.sum(counts)\n",
    "\n",
    "p = list()\n",
    "for r in result:\n",
    "    inx = np.argmax(results == r)\n",
    "    print(f'{r}: {counts[inx]:.2%}')\n",
    "    p.append(counts[inx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "68a818ea200cf26246555ed215b2805781440e4bd9c1dd7c5d891140b3a1e7c0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
